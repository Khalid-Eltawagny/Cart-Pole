{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tflearn\n!pip install pygame","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-08T16:38:48.162099Z","iopub.execute_input":"2022-07-08T16:38:48.162875Z","iopub.status.idle":"2022-07-08T16:39:11.969447Z","shell.execute_reply.started":"2022-07-08T16:38:48.162776Z","shell.execute_reply":"2022-07-08T16:39:11.968122Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import gym\nimport random\nimport numpy as np\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.estimator import regression\nfrom statistics import mean, median\nfrom collections import Counter\nimport pygame","metadata":{"execution":{"iopub.status.busy":"2022-07-08T16:39:11.973059Z","iopub.execute_input":"2022-07-08T16:39:11.973383Z","iopub.status.idle":"2022-07-08T16:39:17.957834Z","shell.execute_reply.started":"2022-07-08T16:39:11.973351Z","shell.execute_reply":"2022-07-08T16:39:17.956752Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"LR = 1e-2\nenv = gym.make(\"CartPole-v1\")\nenv.reset()\ngoal_steps = 500\nscore_requirement = 50\ninitial_games = 10000","metadata":{"execution":{"iopub.status.busy":"2022-07-08T16:39:17.959462Z","iopub.execute_input":"2022-07-08T16:39:17.961710Z","iopub.status.idle":"2022-07-08T16:39:17.979522Z","shell.execute_reply.started":"2022-07-08T16:39:17.961668Z","shell.execute_reply":"2022-07-08T16:39:17.978544Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def initial_population():\n    # [OBS, MOVES]\n    training_data = []\n    # all scores:\n    scores = []\n    # just the scores that met our threshold:\n    accepted_scores = []\n    # iterate through however many games we want:\n    for _ in range(initial_games):\n        score = 0\n        # moves specifically from this environment:\n        game_memory = []\n        # previous observation that we saw\n        prev_observation = []\n        # for each frame in 200\n        for _ in range(goal_steps):\n            # choose random action (0 or 1)\n            action = random.randrange(0,2)\n            # do it!\n            observation, reward, done, info = env.step(action)\n            \n            # notice that the observation is returned FROM the action\n            # so we'll store the previous observation here, pairing\n            # the prev observation to the action we'll take.\n            if len(prev_observation) > 0 :\n                game_memory.append([prev_observation, action])\n            prev_observation = observation\n            score+=reward\n            if done: break\n\n        # IF our score is higher than our threshold, we'd like to save\n        # every move we made\n        # NOTE the reinforcement methodology here. \n        # all we're doing is reinforcing the score, we're not trying \n        # to influence the machine in any way as to HOW that score is \n        # reached.\n        if score >= score_requirement:\n            accepted_scores.append(score)\n            for data in game_memory:\n                # convert to one-hot (this is the output layer for our neural network)\n                if data[1] == 1:\n                    output = [0,1]\n                elif data[1] == 0:\n                    output = [1,0]\n                    \n                # saving our training data\n                training_data.append([data[0], output])\n\n        # reset env to play again\n        env.reset()\n        # save overall scores\n        scores.append(score)\n    \n    # just in case you wanted to reference later\n    training_data_save = np.array(training_data)\n    np.save('saved.npy',training_data_save)\n    \n    # some stats here, to further illustrate the neural network magic!\n    print('Average accepted score:',mean(accepted_scores))\n    print('Median score for accepted scores:',median(accepted_scores))\n    print(Counter(accepted_scores))\n    \n    return training_data","metadata":{"execution":{"iopub.status.busy":"2022-07-08T16:39:17.982506Z","iopub.execute_input":"2022-07-08T16:39:17.983444Z","iopub.status.idle":"2022-07-08T16:39:17.995062Z","shell.execute_reply.started":"2022-07-08T16:39:17.983403Z","shell.execute_reply":"2022-07-08T16:39:17.994122Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def neural_network_model(input_size):\n\n    network = input_data(shape=[None, input_size, 1], name='input')\n\n    network = fully_connected(network, 128, activation='relu')\n    network = dropout(network, 0.8)\n\n    network = fully_connected(network, 256, activation='relu')\n    network = dropout(network, 0.8)\n\n    network = fully_connected(network, 512, activation='relu')\n    network = dropout(network, 0.8)\n\n    network = fully_connected(network, 256, activation='relu')\n    network = dropout(network, 0.8)\n\n    network = fully_connected(network, 128, activation='relu')\n    network = dropout(network, 0.8)\n\n    network = fully_connected(network, 2, activation='softmax')\n    network = regression(network, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n    model = tflearn.DNN(network, tensorboard_dir='log')\n\n    return model\n\n\ndef train_model(training_data, model=False):\n\n    X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n    y = [i[1] for i in training_data]\n    \n    print(len(X))\n\n    if not model:\n        model = neural_network_model(input_size = len(X[0]))\n    \n    model.fit({'input': X}, {'targets': y}, n_epoch=3, snapshot_step=500, show_metric=True, run_id='openai_learning')\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-08T16:39:17.997060Z","iopub.execute_input":"2022-07-08T16:39:17.998205Z","iopub.status.idle":"2022-07-08T16:39:18.011450Z","shell.execute_reply.started":"2022-07-08T16:39:17.998137Z","shell.execute_reply":"2022-07-08T16:39:18.010486Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"training_data = initial_population()","metadata":{"execution":{"iopub.status.busy":"2022-07-08T16:39:18.012804Z","iopub.execute_input":"2022-07-08T16:39:18.013328Z","iopub.status.idle":"2022-07-08T16:39:21.638609Z","shell.execute_reply.started":"2022-07-08T16:39:18.013292Z","shell.execute_reply":"2022-07-08T16:39:21.637614Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = train_model(training_data)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-07-08T16:39:21.640221Z","iopub.execute_input":"2022-07-08T16:39:21.640894Z","iopub.status.idle":"2022-07-08T16:39:34.833892Z","shell.execute_reply.started":"2022-07-08T16:39:21.640855Z","shell.execute_reply":"2022-07-08T16:39:34.832986Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"scores = []\nchoices = []\nfor each_game in range(100):\n    score = 0\n    game_memory = []\n    prev_obs = []\n    env.reset()\n    for _ in range(goal_steps):\n#         env.render()\n\n        if len(prev_obs)==0:\n            action = random.randrange(0,2)\n        else:\n            action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])\n\n        choices.append(action)\n                \n        new_observation, reward, done, info = env.step(action)\n        prev_obs = new_observation\n        game_memory.append([new_observation, action])\n        score+=reward\n        if done: break\n\n    scores.append(score)\n\nprint('Average Score:',sum(scores)/len(scores))\nprint('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\nprint(score_requirement)","metadata":{"execution":{"iopub.status.busy":"2022-07-08T16:39:34.835275Z","iopub.execute_input":"2022-07-08T16:39:34.835986Z","iopub.status.idle":"2022-07-08T16:40:21.091310Z","shell.execute_reply.started":"2022-07-08T16:39:34.835936Z","shell.execute_reply":"2022-07-08T16:40:21.090231Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.save('341.h5')","metadata":{"execution":{"iopub.status.busy":"2022-07-08T16:48:15.342626Z","iopub.execute_input":"2022-07-08T16:48:15.343727Z","iopub.status.idle":"2022-07-08T16:48:15.399687Z","shell.execute_reply.started":"2022-07-08T16:48:15.343678Z","shell.execute_reply":"2022-07-08T16:48:15.398775Z"},"trusted":true},"execution_count":10,"outputs":[]}]}